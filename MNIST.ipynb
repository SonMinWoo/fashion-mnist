{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default MNIST code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2951 - accuracy: 0.9133\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1428 - accuracy: 0.9578\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1066 - accuracy: 0.9672\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0883 - accuracy: 0.9725\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0729 - accuracy: 0.9773\n",
      "313/313 - 0s - loss: 0.0720 - accuracy: 0.9768 - 319ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07195433229207993, 0.9768000245094299]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5266 - accuracy: 0.8152\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3996 - accuracy: 0.8539\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3635 - accuracy: 0.8663\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3417 - accuracy: 0.8737\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3284 - accuracy: 0.8798\n",
      "313/313 - 0s - loss: 0.3593 - accuracy: 0.8697 - 356ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35928475856781006, 0.869700014591217]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not changed model + only data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "from scipy import interpolate\n",
    "import cv2\n",
    "\n",
    "IMG_SIZE = 28\n",
    "\n",
    "def regular(image, label):\n",
    "    img = np.array(image)\n",
    "    img = np.repeat((img/255.0).astype(\"float32\"), 1, -1)\n",
    "    return img, label\n",
    "\n",
    "def edge_detection(image, label):\n",
    "    img =image\n",
    "    laplacian = cv2.Laplacian(img, cv2.CV_8U)\n",
    "    laplacian, _ = regular(laplacian , label)\n",
    "    \n",
    "    edges = cv2.Canny(img, 100, 150)\n",
    "    edges, _ = regular(edges , label)\n",
    "    return (laplacian, edges), label\n",
    "\n",
    "def reverse(image, label): \n",
    "    x, _ = regular(image, \"\")\n",
    "    return x[:,::-1], label\n",
    "\n",
    "def histogram_equal(image, label):\n",
    "    img = cv2.equalizeHist(image)\n",
    "    return img, label\n",
    "\n",
    "def contrast(image, label):\n",
    "    alpha = 1.0\n",
    "    img = np.clip((1+alpha)*image - 128*alpha, 0, 255).astype(np.uint8)\n",
    "    return img, label\n",
    "\n",
    "def shift(image, label):\n",
    "    img, _ = regular(image, \"\")\n",
    "    h, w = img.shape[:2]\n",
    "    weightx = 2.8\n",
    "    weighty = 2.8\n",
    "    if np.random.rand() < 2.5:\n",
    "        weightx *= -1\n",
    "    elif np.random.rand() < 5.0:\n",
    "        weighty *= -1\n",
    "    elif np.random.rand() < 7.5:\n",
    "        weightx *= -1\n",
    "        weighty *= -1\n",
    "    M = np.float32([[1, 0, weightx], [0, 1, weighty]])\n",
    "    img2 = cv2.warpAffine(img, M, (w, h))\n",
    "    return img2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23f345da490>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(contrast(train_images[0], \"\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(train_images)):\\n    tmpx, tmpy = shift(train_images[i], train_labels[i])\\n    x_train.append(tmpx)\\n    y_train.append(tmpy)'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = list(), list()\n",
    "\n",
    "#1 기본 정규화 코드\n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = regular(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)\n",
    "    \n",
    "#2 data augmentation : flip - 2와 결과 비슷\n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = reverse(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)\n",
    "\n",
    "    \n",
    "#3 feature engineering : 0.889로 살짝 더 결과가 좋음 \n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = edge_detection(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx[0])\n",
    "    y_train.append(tmpy)\n",
    "    x_train.append(tmpx[1])\n",
    "    y_train.append(tmpy)\n",
    "    \n",
    "\n",
    "    \n",
    "#4 feature engineering : 0.72로 결과 안좋다. 폐기\n",
    "\"\"\"for i in range(len(train_images)):\n",
    "    tmpx, tmpy = histogram_equal(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy) \n",
    "\n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = reverse(train_images[i], train_labels[i])\n",
    "    tmpx, _ = histogram_equal(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)\"\"\"  \n",
    "   \n",
    "#5 feature engineering : 0.71로 결과 안좋다. 폐기\n",
    "\"\"\"for i in range(len(train_images)):\n",
    "    tmpx, tmpy = contrast(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy) \n",
    "\n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = reverse(train_images[i], train_labels[i])\n",
    "    tmpx, _ = contrast(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)  \"\"\"\n",
    "    \n",
    "#6 data augmentation : shift 0.888 결과좋다!\n",
    "\n",
    "\"\"\"for i in range(len(train_images)):\n",
    "    tmpx, tmpy = shift(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)\"\"\"\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = tf.convert_to_tensor(x_train),tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18750/18750 [==============================] - 34s 2ms/step - loss: 0.4279 - accuracy: 0.8454\n",
      "Epoch 2/5\n",
      "18750/18750 [==============================] - 34s 2ms/step - loss: 0.3417 - accuracy: 0.8758\n",
      "Epoch 3/5\n",
      "18750/18750 [==============================] - 33s 2ms/step - loss: 0.3143 - accuracy: 0.8851\n",
      "Epoch 4/5\n",
      "18750/18750 [==============================] - 34s 2ms/step - loss: 0.2972 - accuracy: 0.8917\n",
      "Epoch 5/5\n",
      "18750/18750 [==============================] - 35s 2ms/step - loss: 0.2853 - accuracy: 0.8955\n",
      "313/313 - 0s - loss: 0.3376 - accuracy: 0.8861 - 381ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33764564990997314, 0.8860999941825867]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.3415 - accuracy: 0.8758\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.2336 - accuracy: 0.9130\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.1945 - accuracy: 0.9273\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.1646 - accuracy: 0.9378\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.1393 - accuracy: 0.9476\n",
      "313/313 - 2s - loss: 0.3064 - accuracy: 0.9012 - 2s/epoch - 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3063916563987732, 0.901199996471405]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(32, (3, 3), strides=1, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, 'relu'),\n",
    "    Dense(10, 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "x_train, y_train = list(), list()\n",
    "\n",
    "#1 기본 정규화 코드\n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = regular(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)\n",
    "    \n",
    "#2 data augmentation : flip - 2와 결과 비슷\n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = reverse(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx)\n",
    "    y_train.append(tmpy)\n",
    "\n",
    "    \n",
    "#3 feature engineering : 0.889로 살짝 더 결과가 좋음 \n",
    "for i in range(len(train_images)):\n",
    "    tmpx, tmpy = edge_detection(train_images[i], train_labels[i])\n",
    "    x_train.append(tmpx[0])\n",
    "    y_train.append(tmpy)\n",
    "    x_train.append(tmpx[1])\n",
    "    y_train.append(tmpy)\n",
    "    \n",
    "x_train, y_train = tf.convert_to_tensor(x_train),tf.convert_to_tensor(y_train)\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7500/7500 [==============================] - 174s 23ms/step - loss: 0.3283 - accuracy: 0.8780\n",
      "Epoch 2/5\n",
      "7500/7500 [==============================] - 167s 22ms/step - loss: 0.2336 - accuracy: 0.9130\n",
      "Epoch 3/5\n",
      "7500/7500 [==============================] - 168s 22ms/step - loss: 0.1976 - accuracy: 0.9257\n",
      "Epoch 4/5\n",
      "7500/7500 [==============================] - 171s 23ms/step - loss: 0.1710 - accuracy: 0.9350\n",
      "Epoch 5/5\n",
      "7500/7500 [==============================] - 167s 22ms/step - loss: 0.1501 - accuracy: 0.9427\n",
      "313/313 - 2s - loss: 0.2506 - accuracy: 0.9197 - 2s/epoch - 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25058045983314514, 0.919700026512146]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(32, (3, 3), strides=1, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, 'relu'),\n",
    "    Dense(10, 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
